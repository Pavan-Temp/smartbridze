# -*- coding: utf-8 -*-
"""Sustainable_Smart_City

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16sGWQmN0uPJUN5gsXpli2bfI8SMojUQr
"""



# Import required libraries
import os
import torch
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from datetime import datetime, timedelta
import json
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
import threading
import time
from pyngrok import ngrok

# Check GPU availability
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

class SmartCityAssistant:
    def __init__(self, hf_token):
        """Initialize the Smart City Assistant with IBM Granite model"""
        self.hf_token = hf_token
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Initialize model and tokenizer globally (one-time loading)
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.tokenizer = None
        self.model = None
        self.generator = None

        # Storage for reports and data
        self.citizen_reports = []
        self.kpi_data = {}

        self.load_model()

    def load_model(self):
        """Load IBM Granite model globally for faster inference"""
        try:
            print("Loading IBM Granite model...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token,
                trust_remote_code=True
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                token=self.hf_token,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )

            # Create text generation pipeline
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map="auto",
                torch_dtype=torch.float16,
                do_sample=True,
                temperature=0.7,
                max_new_tokens=512
            )

            print("Model loaded successfully!")

        except Exception as e:
            print(f"Error loading model: {e}")
            # Fallback to a smaller model if Granite fails
            print("Falling back to distilgpt2...")
            self.generator = pipeline("text-generation", model="distilgpt2", device=0 if torch.cuda.is_available() else -1)

    def generate_response(self, prompt, max_tokens=300):
        """Generate response using IBM Granite LLM"""
        try:
            # Format prompt for instruction-following
            formatted_prompt = f"### Instruction:\n{prompt}\n\n### Response:\n"

            response = self.generator(
                formatted_prompt,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                pad_token_id=self.tokenizer.eos_token_id if self.tokenizer else None
            )

            # Extract only the response part
            generated_text = response[0]['generated_text']
            response_part = generated_text.split("### Response:\n")[-1].strip()

            return response_part

        except Exception as e:
            print(f"Error generating response: {e}")
            return "I apologize, but I'm experiencing technical difficulties. Please try again."

    def policy_summarization(self, policy_text):
        """Summarize complex policy documents"""
        prompt = f"""
        Summarize the following city policy document in citizen-friendly language.
        Make it concise and highlight key points that affect residents:

        {policy_text[:2000]}  # Limit input length

        Provide a summary with:
        1. Main objectives
        2. Key changes for citizens
        3. Implementation timeline
        """

        return self.generate_response(prompt, max_tokens=400)

    def process_citizen_feedback(self, report_data):
        """Process and categorize citizen feedback reports"""
        categories = {
            'water': ['water', 'pipe', 'leak', 'drainage', 'sewage'],
            'traffic': ['traffic', 'road', 'signal', 'parking', 'accident'],
            'environment': ['waste', 'pollution', 'noise', 'air', 'garbage'],
            'infrastructure': ['street', 'light', 'sidewalk', 'building', 'construction'],
            'safety': ['crime', 'safety', 'police', 'emergency', 'security']
        }

        # Auto-categorize based on keywords
        description = report_data.get('description', '').lower()
        category = 'general'

        for cat, keywords in categories.items():
            if any(keyword in description for keyword in keywords):
                category = cat
                break

        # Generate automated response
        prompt = f"""
        A citizen reported the following issue: {report_data.get('description', '')}
        Location: {report_data.get('location', 'Not specified')}

        Provide a professional acknowledgment response and suggest immediate actions.
        """

        ai_response = self.generate_response(prompt, max_tokens=200)

        # Store report
        report = {
            'id': len(self.citizen_reports) + 1,
            'timestamp': datetime.now().isoformat(),
            'category': category,
            'description': report_data.get('description'),
            'location': report_data.get('location'),
            'contact': report_data.get('contact'),
            'priority': self.assess_priority(description),
            'ai_response': ai_response,
            'status': 'pending'
        }

        self.citizen_reports.append(report)
        return report

    def assess_priority(self, description):
        """Assess priority of citizen reports"""
        high_priority_keywords = ['emergency', 'burst', 'fire', 'accident', 'danger', 'urgent']
        medium_priority_keywords = ['broken', 'damaged', 'blocked', 'overflow']

        description_lower = description.lower()

        if any(keyword in description_lower for keyword in high_priority_keywords):
            return 'high'
        elif any(keyword in description_lower for keyword in medium_priority_keywords):
            return 'medium'
        else:
            return 'low'

    def kpi_forecasting(self, csv_data, kpi_type):
        """Forecast KPI values using machine learning"""
        try:
            # Parse CSV data
            df = pd.read_csv(pd.StringIO(csv_data))

            # Prepare data for forecasting
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df = df.sort_values('date')

                # Create features
                df['month'] = df['date'].dt.month
                df['year'] = df['date'].dt.year
                df['day_of_year'] = df['date'].dt.dayofyear

                # Assume the last column is the target KPI
                target_col = df.columns[-1]
                feature_cols = ['month', 'year', 'day_of_year']

                X = df[feature_cols].values
                y = df[target_col].values

                # Train simple linear regression
                model = LinearRegression()
                model.fit(X, y)

                # Forecast next 12 months
                last_date = df['date'].max()
                forecasts = []

                for i in range(1, 13):
                    future_date = last_date + timedelta(days=30*i)
                    features = [future_date.month, future_date.year, future_date.timetuple().tm_yday]
                    prediction = model.predict([features])[0]

                    forecasts.append({
                        'date': future_date.strftime('%Y-%m-%d'),
                        'predicted_value': round(prediction, 2),
                        'kpi_type': kpi_type
                    })

                # Generate insights using AI
                avg_historical = np.mean(y)
                avg_forecast = np.mean([f['predicted_value'] for f in forecasts])
                trend = "increasing" if avg_forecast > avg_historical else "decreasing"

                prompt = f"""
                Analyze the {kpi_type} KPI forecast results:
                - Historical average: {avg_historical:.2f}
                - Forecasted average: {avg_forecast:.2f}
                - Trend: {trend}

                Provide insights and recommendations for city planning.
                """

                insights = self.generate_response(prompt, max_tokens=300)

                return {
                    'forecasts': forecasts,
                    'insights': insights,
                    'trend': trend,
                    'accuracy_score': 'Based on historical data patterns'
                }

        except Exception as e:
            return {'error': f'Error processing KPI data: {str(e)}'}

    def generate_eco_tips(self, keywords):
        """Generate eco-friendly tips based on keywords"""
        prompt = f"""
        Generate 5 practical and actionable eco-friendly tips related to: {', '.join(keywords)}

        Make the tips specific, easy to implement, and suitable for city residents.
        Include both individual actions and community-level suggestions.
        """

        return self.generate_response(prompt, max_tokens=400)

    def anomaly_detection(self, csv_data):
        """Detect anomalies in KPI data"""
        try:
            df = pd.read_csv(pd.StringIO(csv_data))

            # Assume last column is the KPI value
            kpi_col = df.columns[-1]
            values = df[kpi_col].values.reshape(-1, 1)

            # Standardize data
            scaler = StandardScaler()
            values_scaled = scaler.fit_transform(values)

            # Detect anomalies using Isolation Forest
            detector = IsolationForest(contamination=0.1, random_state=42)
            anomalies = detector.fit_predict(values_scaled)

            # Identify anomalous records
            anomaly_indices = np.where(anomalies == -1)[0]
            anomaly_records = []

            for idx in anomaly_indices:
                record = df.iloc[idx].to_dict()
                record['anomaly_score'] = abs(values_scaled[idx][0])
                anomaly_records.append(record)

            # Generate AI analysis
            if anomaly_records:
                anomaly_values = [record[kpi_col] for record in anomaly_records]
                prompt = f"""
                Anomalies detected in city KPI data:
                - Anomalous values: {anomaly_values}
                - Normal range average: {np.mean(values):.2f}

                Analyze these anomalies and suggest possible causes and actions for city administrators.
                """

                analysis = self.generate_response(prompt, max_tokens=300)
            else:
                analysis = "No significant anomalies detected in the provided data."

            return {
                'anomalies_found': len(anomaly_records),
                'anomaly_records': anomaly_records,
                'analysis': analysis,
                'total_records': len(df)
            }

        except Exception as e:
            return {'error': f'Error detecting anomalies: {str(e)}'}

    def chat_assistant(self, message):
        """General chat assistant for city-related queries"""
        prompt = f"""
        You are a helpful Smart City Assistant. Answer the following question about urban planning,
        sustainability, city services, or civic matters:

        Question: {message}

        Provide a comprehensive and practical answer.
        """

        return self.generate_response(prompt, max_tokens=400)

    def traffic_route_suggestion(self, origin, destination, city):
        """Generate traffic route suggestions and famous places"""
        prompt = f"""
        A visitor is traveling from {origin} to {destination} in {city}.

        Provide:
        1. Suggested route with less traffic (general directions)
        2. Famous places/attractions near the destination
        3. Best time to travel to avoid traffic
        4. Local transportation options
        """

        return self.generate_response(prompt, max_tokens=400)

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Global assistant instance
assistant = None

def initialize_assistant():
    """Initialize the assistant with Hugging Face token"""
    global assistant
    HF_TOKEN = os.getenv("HF_TOKEN")
    assistant = SmartCityAssistant(HF_TOKEN)

# API Routes
@app.route('/api/policy-summary', methods=['POST'])
def policy_summary():
    data = request.json
    policy_text = data.get('policy_text', '')

    if not policy_text:
        return jsonify({'error': 'Policy text is required'}), 400

    summary = assistant.policy_summarization(policy_text)
    return jsonify({'summary': summary})

@app.route('/api/citizen-report', methods=['POST'])
def citizen_report():
    data = request.json
    report = assistant.process_citizen_feedback(data)
    return jsonify(report)

@app.route('/api/kpi-forecast', methods=['POST'])
def kpi_forecast():
    data = request.json
    csv_data = data.get('csv_data', '')
    kpi_type = data.get('kpi_type', 'general')

    if not csv_data:
        return jsonify({'error': 'CSV data is required'}), 400

    forecast = assistant.kpi_forecasting(csv_data, kpi_type)
    return jsonify(forecast)

@app.route('/api/eco-tips', methods=['POST'])
def eco_tips():
    data = request.json
    keywords = data.get('keywords', [])

    if not keywords:
        return jsonify({'error': 'Keywords are required'}), 400

    tips = assistant.generate_eco_tips(keywords)
    return jsonify({'tips': tips})

@app.route('/api/anomaly-detection', methods=['POST'])
def anomaly_detection():
    data = request.json
    csv_data = data.get('csv_data', '')

    if not csv_data:
        return jsonify({'error': 'CSV data is required'}), 400

    result = assistant.anomaly_detection(csv_data)
    return jsonify(result)

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    message = data.get('message', '')

    if not message:
        return jsonify({'error': 'Message is required'}), 400

    response = assistant.chat_assistant(message)
    return jsonify({'response': response})

@app.route('/api/traffic-route', methods=['POST'])
def traffic_route():
    data = request.json
    origin = data.get('origin', '')
    destination = data.get('destination', '')
    city = data.get('city', '')

    if not all([origin, destination, city]):
        return jsonify({'error': 'Origin, destination, and city are required'}), 400

    suggestions = assistant.traffic_route_suggestion(origin, destination, city)
    return jsonify({'suggestions': suggestions})

@app.route('/api/reports', methods=['GET'])
def get_reports():
    return jsonify(assistant.citizen_reports)

@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'model_loaded': assistant is not None})

# Start the Flask app
def run_app():
    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)

# Cell 4: Start the server with ngrok (FIXED VERSION)
def start_server():
    """Start Flask server with ngrok tunnel"""
    # Initialize the assistant
    print("üöÄ Initializing Smart City Assistant...")
    initialize_assistant()

    # Set ngrok authtoken (REQUIRED)
    # REPLACE WITH YOUR ACTUAL NGROK AUTHTOKEN
    NGROK_TOKEN = os.getenv("NGROK_TOKEN")
    ngrok.set_auth_token(NGROK_TOKEN)

    # Start ngrok tunnel
    print("üåê Creating ngrok tunnel...")
    try:
        public_url = ngrok.connect(5000)
        print(f"‚úÖ Public URL: {public_url}")

        # Update frontend API_BASE URL
        print(f"""
        üìù UPDATE YOUR FRONTEND:
        Replace 'http://localhost:5000/api' with '{public_url}/api' in your HTML file
        """)
    except Exception as e:
        print(f"‚ùå ngrok error: {e}")
        print("üîÑ Falling back to local hosting only...")
        print("Frontend will be available at: http://localhost:5000")

    # Start Flask app
    print("üñ•Ô∏è Starting Flask server...")
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)

# Run the server
start_server()

# Cell: Backend Verification
print("=== BACKEND STATUS CHECK ===")
print(f"‚úÖ Flask app created: {app is not None}")
print(f"‚úÖ Assistant initialized: {assistant is not None}")
print(f"‚úÖ Model loaded: {assistant.model is not None if assistant else False}")
print(f"‚úÖ GPU available: {torch.cuda.is_available()}")

# Test the health endpoint locally
with app.test_client() as client:
    response = client.get('/api/health')
    print(f"‚úÖ Health endpoint: {response.status_code}")
    if response.status_code == 200:
        print(f"   Response: {response.get_json()}")

print("\n=== NGROK STATUS ===")
try:
    tunnels = ngrok.get_tunnels()
    for tunnel in tunnels:
        print(f"‚úÖ Tunnel active: {tunnel.public_url}")
except:
    print("‚ùå No active ngrok tunnels found")

# ngrok Setup & Fix
from pyngrok import ngrok
import time

# REPLACE WITH YOUR ACTUAL NGROK TOKEN
ngrok_token = "2ypRPxyDYuES00zDxcQN1J3B9hr_5PzCGfzCAFDg7wE5j27Fd"  # ‚ö†Ô∏è CHANGE THIS

# Configure ngrok
ngrok.set_auth_token(ngrok_token)

# Kill any existing processes
try:
    ngrok.kill()
    time.sleep(2)
except:
    pass

# Create new tunnel
public_url = ngrok.connect(5000)
print(f"‚úÖ SUCCESS! Your backend is now live at:")
print(f"üîó {public_url}")
print(f"\nüìù UPDATE YOUR HTML FILE:")
print(f"Replace: const API_BASE = 'http://localhost:5000/api';")
print(f"With:    const API_BASE = '{public_url}/api';")

# Verify it's working
tunnels = ngrok.get_tunnels()
print(f"\nüîç Active tunnels: {len(tunnels)}")
for tunnel in tunnels:
    print(f"   - {tunnel.public_url}")
